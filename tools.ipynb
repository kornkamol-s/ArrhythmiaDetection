{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "4pJGFEYpNt9g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2024-04-03 10:53:05 +01:00)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import joblib\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.callbacks import EpochScoring\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp():\n",
    "    \"\"\"\n",
    "        Get current timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%dT%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "M1vnOhURQADc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-04-03 10:53:05 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def _import_data(path, validation_size=None):\n",
    "    \"\"\"\n",
    "        Import source data\n",
    "    \"\"\"\n",
    "\n",
    "    # Read source files\n",
    "    df = pd.read_csv(f'source/mitbih_{path}.csv', header=None)\n",
    "\n",
    "    # Extract data, and labels\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values.astype('int64')\n",
    "\n",
    "    # Split into validation set, if needed\n",
    "    if validation_size:\n",
    "        X1, X2, y1, y2 = train_test_split(X, y, test_size=validation_size, random_state=42)\n",
    "        \n",
    "        return X1, y1, X2, y2\n",
    "        \n",
    "    else:\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2024-04-03 10:53:06 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def _gaussian_noise(X_train):\n",
    "    \"\"\"\n",
    "        Add noise to dataset\n",
    "    \"\"\"\n",
    "\n",
    "    noise = np.random.normal(loc=0, scale=0.03, size=X_train.shape)\n",
    "\n",
    "    return X_train + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-04-03 10:53:06 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def _balancing(X, y, num_sample):\n",
    "    \"\"\"\n",
    "        Balancing data with specific number of records\n",
    "    \"\"\"\n",
    "\n",
    "    # Get records count\n",
    "    label, count = np.unique(y, return_counts=True)\n",
    "    \n",
    "    X_balanced = []\n",
    "    y_balanced = []\n",
    "    \n",
    "    for lbl, cnt in zip(label, count):\n",
    "        X_filter = X[y==lbl]\n",
    "        y_filter = y[y==lbl]\n",
    "\n",
    "        # Downsampling if data exceeds desire number\n",
    "        if cnt > num_sample:\n",
    "            X_filter, y_filter = resample(X_filter, y_filter, \n",
    "                                          replace=False,\n",
    "                                          n_samples=num_sample,\n",
    "                                          random_state=42)\n",
    "\n",
    "        # Otherwise, upsampling with bootstrap\n",
    "        elif cnt < num_sample:\n",
    "            X_filter, y_filter = resample(X_filter, y_filter, \n",
    "                                          replace=True,\n",
    "                                          n_samples=num_sample,\n",
    "                                          random_state=42)\n",
    "        X_balanced.append(X_filter)   \n",
    "        y_balanced.append(y_filter)\n",
    "        \n",
    "    X_balanced = np.concatenate(X_balanced, axis=0)\n",
    "    y_balanced = np.concatenate(y_balanced, axis=0)\n",
    "    \n",
    "    return X_balanced, y_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-04-03 10:53:06 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def _get_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        Generate classification report\n",
    "    \"\"\"\n",
    "\n",
    "    report = classification_report(y_true, y_pred)\n",
    "                          \n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _roc_curve(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Generate ROC curve\n",
    "    Code for generating ROC curve obtained from documentation : \n",
    "    https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert true labels to one-hot encoding\n",
    "    y_true = LabelBinarizer().fit_transform(y_true)\n",
    "\n",
    "    # Define the number of classes\n",
    "    n_classes = 5\n",
    "    class_labels = {0: 'N', 1: 'S', 2: 'V', 3: 'F', 4: 'Q'}\n",
    "\n",
    "    # Define colors for each class\n",
    "    colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\", \"olive\", \"maroon\"])\n",
    "\n",
    "    # Initialize dictionaries to store fpr, tpr, and roc_auc for each class\n",
    "    fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "\n",
    "    # Compute micro-average ROC\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_pred.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Compute macro-average ROC \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    fpr_grid = np.linspace(0.0, 1.0, 1000)\n",
    "    \n",
    "    # Interpolate all ROC curves at these points\n",
    "    mean_tpr = np.zeros_like(fpr_grid)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])  # linear interpolation\n",
    "\n",
    "    # Average interpolated TPRs and compute macro AUC\n",
    "    mean_tpr /= n_classes\n",
    "    \n",
    "    fpr[\"macro\"] = fpr_grid\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Set the figure size\n",
    "    fig.set_size_inches(8, 6)\n",
    "\n",
    "    # Plot micro-average ROC curve\n",
    "    plt.plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=f\"micro-average ROC curve (AUC = {roc_auc['micro']:.2f})\",\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    # Plot macro-average ROC curve\n",
    "    plt.plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=f\"macro-average ROC curve (AUC = {roc_auc['macro']:.2f})\",\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    # Plot individual ROC curves for each class\n",
    "    for class_id, color in zip(range(n_classes), colors):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_true[:, class_id],\n",
    "            y_pred[:, class_id],\n",
    "            name=f\"ROC curve for Class {class_labels[class_id]}\",\n",
    "            color=color,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "    # Set plot labels and title\n",
    "    ax.set(\n",
    "        xlabel=\"False Positive Rate\",\n",
    "        ylabel=\"True Positive Rate\",\n",
    "        title=\" Receiver Operation Curve\",\n",
    "    )\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_confusion_matrix(y_true, y_pred, title=None):\n",
    "    \"\"\"\n",
    "        Generate confusion matrix\n",
    "    \"\"\"\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    class_labels = ['N', 'S', 'V', 'F', 'Q']\n",
    "\n",
    "    # Calculate counts for each class\n",
    "    class_totals = cm.sum(axis=1)\n",
    "\n",
    "    # Calculate percentage for each class\n",
    "    cm_percent = (cm.T / class_totals).T * 100\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    # Plot confusion matrix with heatmap\n",
    "    sns.heatmap(cm_percent, annot=False, cmap=\"Blues\", fmt='d', xticklabels=class_labels, yticklabels=class_labels, cbar=False, linewidths=1, linecolor='white')\n",
    "\n",
    "    # Annotate with total predictions\n",
    "    for i in range(len(class_labels)):\n",
    "        for j in range(len(class_labels)):\n",
    "            # Annotations for count\n",
    "            plt.text(j + 0.5, i + 0.6, f'{cm[i, j]}', ha='center', va='center', color='black', fontsize=8)\n",
    "            # Annotations for percentage\n",
    "            plt.text(j + 0.5, i + 0.4, f'{cm_percent[i, j]:.2f}%', ha='center', va='center', color='black', fontsize=8)\n",
    "\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(title)\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-04-03 10:53:30 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def _convert_to_tensor(X, y):\n",
    "    \"\"\"\n",
    "        Convert data to tensor dataset\n",
    "    \"\"\"\n",
    "\n",
    "    X = X.reshape(-1, 1, X.shape[-1])\n",
    "    \n",
    "    X = torch.from_numpy(X).float()\n",
    "    y = torch.from_numpy(y).long()\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-04-03 10:53:30 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def _load_cnn_model(cnn, fn):\n",
    "    \"\"\"\n",
    "        Load model for evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(f\"models/cnn/{fn}.pth\", map_location=device)\n",
    "\n",
    "    # Initialize model\n",
    "    model = cnn()\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # Update model parameters with checkpoint values\n",
    "    for key in checkpoint.keys():\n",
    "        if key in model_dict:\n",
    "            model_dict[key] = checkpoint[key]\n",
    "\n",
    "    # Load updated parameters into the model\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    # Move model to cuda (if available)\n",
    "    model.to(device)\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-04-03 10:53:30 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def _evaluate_cnn(cnn, fn, subset='test', roc_curve=True):\n",
    "    \"\"\"\n",
    "        Evaluate CNN model\n",
    "                                \n",
    "        Params: \n",
    "        ------------------------------------------------------------------------------\n",
    "            cnn - Model Class (CNN or ResCNN)\n",
    "            fn - Filename of model to be evaluated\n",
    "            subset - Subset of source data to be evaluated (train, validation, test)\n",
    "            roc_curve - Whether to show ROC curve\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    # Load model\n",
    "    model = _load_cnn_model(cnn, fn)\n",
    "\n",
    "\n",
    "    if subset == 'test':\n",
    "        # Load data\n",
    "        X_test, y_test = _import_data('test')\n",
    "\n",
    "        # Preprocess data\n",
    "        X, y = _preprocess(X_test, y_test)\n",
    "\n",
    "    else:\n",
    "        # Load data\n",
    "        X_train, y_train, X_val, y_val = _import_data('train', validation_size=0.2)\n",
    "\n",
    "        if subset == 'train':\n",
    "            X, y = _preprocess(X_train, y_train)\n",
    "\n",
    "        else:\n",
    "            X, y = _preprocess(X_val, y_val)\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    # Evaluate, and predict with probability\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        \n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Predicting time: {end-start}\")\n",
    "    \n",
    "    # Get predicted labels with highest probability\n",
    "    _, y_pred = torch.max(outputs, 1)\n",
    "\n",
    "    # Get classification report\n",
    "    _get_report(y.cpu(), y_pred.cpu())\n",
    "\n",
    "    # Generate confusion matric\n",
    "    _get_confusion_matrix(y.cpu(), y_pred.cpu())\n",
    "\n",
    "    if roc_curve:\n",
    "        # Plot ROC curve\n",
    "        _roc_curve(y.detach().cpu().numpy(), outputs.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-04-03 10:53:30 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def _initilise_cnn(model, **kwargs):\n",
    "    \"\"\"\n",
    "        Initialise CNN model using skorch, NeuralNetClassifier\n",
    "    \"\"\"\n",
    "\n",
    "    # Define batchsize, epoch, and loss function\n",
    "    return NeuralNetClassifier(\n",
    "                            model,\n",
    "                            criterion=nn.CrossEntropyLoss,\n",
    "                            device=device,\n",
    "                            verbose=True,\n",
    "                            max_epochs=100,\n",
    "                            batch_size=128,\n",
    "                            **kwargs\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-04-03 10:53:30 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def _callbacks(earlystop_patience=10, lr_scheduler=None, checkpoint=True):\n",
    "    \"\"\"\n",
    "        Define callbacks for model training\n",
    "    \"\"\"\n",
    "\n",
    "    # Earlystopping to prevent overfitting, by stop training when validation loss does not improve more than threshold\n",
    "    early_stop = EarlyStopping(monitor='valid_loss', patience=earlystop_patience)\n",
    "\n",
    "    # Model checkpoint to continuosly save best model, with the focus on best validation loss\n",
    "    model_path = f'models/cnn/{get_timestamp()}.pth'\n",
    "    checkpoint = Checkpoint(\n",
    "        f_params=model_path,\n",
    "        monitor='valid_loss_best',\n",
    "        f_optimizer=None,\n",
    "        f_history=None,\n",
    "        f_criterion=None\n",
    "    )\n",
    "\n",
    "    # Define callback to compute and log training accuracy\n",
    "    train_acc = EpochScoring(scoring='accuracy', name='train_acc', on_train=True)\n",
    "    \n",
    "    if checkpoint:\n",
    "        return [early_stop, lr_scheduler, checkpoint, train_acc]\n",
    "    \n",
    "    else:\n",
    "        return [early_stop, lr_scheduler, train_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gridsearchcv(X_train, y_train, model, param_grid, cv=5, scoring='f1_macro'):\n",
    "    \"\"\"\n",
    "        Perform paremeter tuning with stratify k-fold cross validation\n",
    "    \"\"\"\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=skf, scoring=scoring, n_jobs=-1, verbose=0)\n",
    "    grid_result = grid.fit(X_train.cpu(), y_train.cpu())\n",
    "    \n",
    "    return grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-04-03 10:53:30 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def _plot_history(history):\n",
    "    \"\"\"\n",
    "        Plot training and validation loss/accuracy over epochs\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    axs[0].plot(history[:, 'train_loss'], label='Training')\n",
    "    axs[0].plot(history[:, 'valid_loss'], label='Validation')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(history[:, 'train_acc'], label='Training')\n",
    "    axs[1].plot(history[:, 'valid_acc'], label='Validation')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-04-03 10:53:30 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def _preprocess(X, y, balance=False, noise=False):\n",
    "    \"\"\"\n",
    "    Preprocess data with optional balancing and augmentation, \n",
    "    then convert to tensor dataset and move to CUDA (if available)\n",
    "    \"\"\"\n",
    "\n",
    "    # Balance data if specified\n",
    "    if balance:\n",
    "        X, y = _balancing(X, y, balance)\n",
    "\n",
    "    # Add noise for augmentation if specified\n",
    "    if noise:\n",
    "        X = _gaussian_noise(X)\n",
    "\n",
    "    # Convert data to tensor dataset\n",
    "    X, y = _convert_to_tensor(X, y)\n",
    "\n",
    "    # Move data to CUDA device if available\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-04-03 10:53:30 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def _cnn_pipeline_with_gridsearch(cnn, param_grid, earlystop_patience=10, \n",
    "                                  checkpoint=False, balance=False, noise=False, **kwargs):\n",
    "    \"\"\"\n",
    "        Encapsulated CNN pipeline for importing training data, \n",
    "        preprocessing, hyperparameter tuning, \n",
    "        and returning the best parameters\n",
    "        \n",
    "        Params: \n",
    "        ------------------------------------------------------------------------------\n",
    "            cnn - Model Class (CNN or ResCNN)\n",
    "            param_grid - Dictionary of parameters to be selected through gridsearch process\n",
    "            earlystop_patience - Early stopping threshold\n",
    "            checkpoint - Whether to enable checkpoint to continuously save best model during training\n",
    "            balance - Whether to balance dataset or not\n",
    "            noise - Whether to add noise to dataset or not\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Import training data, and filter out validation set\n",
    "    X_train, y_train, X_val, y_val = _import_data('train', validation_size=0.2)    \n",
    "\n",
    "    # Preproces train and validation set\n",
    "    X_train, y_train = _preprocess(X_train, y_train, balance=balance, noise=noise)\n",
    "\n",
    "    # Define scheduler to adjust learning rate during training\n",
    "    lr_scheduler = LRScheduler(policy='ReduceLROnPlateau', mode='min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "    # Define callbacks for earlystopping and learning rate scheduler\n",
    "    callbacks = _callbacks(earlystop_patience=earlystop_patience, lr_scheduler=lr_scheduler)\n",
    "\n",
    "    # Initialize CNN model\n",
    "    model = _initilise_cnn(cnn, callbacks=callbacks, optimizer=optim.SGD, optimizer__momentum=0.9, \n",
    "                           optimizer__weight_decay=0.0001, lr=0.05, **kwargs)\n",
    "    model.initialize()\n",
    "\n",
    "    # Move to cuda (if available)\n",
    "    model.module_.to(device)\n",
    "\n",
    "    # Perform gridsearch cross-validation\n",
    "    grid_result = _gridsearchcv(X_train, y_train, model, param_grid, cv=5)\n",
    "\n",
    "    # Get best params and scores\n",
    "    best_params = grid_result.best_params_\n",
    "    best_score = grid_result.best_score_\n",
    "    print(\"Best score: %f with %s\" % (best_score, best_params))\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-04-03 10:53:30 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def _cnn_pipeline_with_best_param(cnn, params, earlystop_patience=10, \n",
    "                                  checkpoint=True, class_weight=False, balance=False, \n",
    "                                  noise=False, fn=None, optimizer=None, lr_scheduler=None):\n",
    "    \"\"\"\n",
    "        Encapsulated CNN pipeline for training model with best parameters\n",
    "                \n",
    "        Params: \n",
    "        ------------------------------------------------------------------------------\n",
    "            cnn - Model Class (CNN or ResCNN)\n",
    "            params - Dictionary of best parameters obtained from parameter selection process\n",
    "            earlystop_patience - Early stopping threshold\n",
    "            checkpoint - Whether to enable checkpoint to continuously save best model during training\n",
    "            balance - Whether to balance dataset or not\n",
    "            noise - Whether to add noise to dataset or not\n",
    "            fn - Customized filename of final model will be saved to\n",
    "            optimizer - Whether to use default optimizer settings, or manually passes\n",
    "            lr_scheduler - Learning rate scheduler\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    # Import train and validation set\n",
    "    X_train, y_train, X_val, y_val = _import_data('train', validation_size=0.2)\n",
    "\n",
    "    # Compute class weight with class frequency to handle class imbalanced\n",
    "    weights = torch.tensor(compute_class_weight('balanced', \n",
    "                                        classes=np.unique(y_train), \n",
    "                                        y=y_train.flatten()), dtype=torch.float)\n",
    "\n",
    "    # Preprocess train and validation set\n",
    "    X_train, y_train = _preprocess(X_train, y_train, balance=balance, noise=noise)\n",
    "    X_val, y_val = _preprocess(X_val, y_val, balance=balance, noise=noise)\n",
    "\n",
    "\n",
    "    # Define scheduler to adjust learning rate during training\n",
    "    if lr_scheduler is None:\n",
    "        lr_scheduler = LRScheduler(policy='ReduceLROnPlateau', mode='min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = _callbacks(earlystop_patience=earlystop_patience, lr_scheduler=lr_scheduler, checkpoint=checkpoint)\n",
    "\n",
    "    # Initialize CNN model\n",
    "    if optimizer is None:\n",
    "        if class_weight:\n",
    "            model = _initilise_cnn(cnn, callbacks=callbacks, train_split=predefined_split(Dataset(X_val, y_val)), \n",
    "                                   criterion__weight=weights, optimizer=optim.SGD, optimizer__momentum=0.9, optimizer__weight_decay=0.0001, lr=0.05,**params)\n",
    "            \n",
    "        else:\n",
    "            model = _initilise_cnn(cnn, callbacks=callbacks, train_split=predefined_split(Dataset(X_val, y_val)), \n",
    "                                   optimizer=optim.SGD, optimizer__momentum= 0.9, optimizer__weight_decay=0.0001, lr=0.05,**params)\n",
    "\n",
    "    # To train model with replicate structure from reference paper\n",
    "    else:\n",
    "        model = _initilise_cnn(cnn, callbacks=callbacks, criterion__weight=weights, train_split=predefined_split(Dataset(X_val, y_val)), **params)\n",
    "    \n",
    "    model.initialize()\n",
    "\n",
    "    # Move model to cuda if available\n",
    "    model.module_.to(device)\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Get model prediction on train set\n",
    "    y_pred = model.predict(X_train)\n",
    "\n",
    "    # Generate classification report for train set\n",
    "    _get_report(y_train.cpu().numpy(), y_pred)\n",
    "\n",
    "    # Get model prediction on validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Generate classification report for validation set\n",
    "    _get_report(y_val.cpu().numpy(), y_pred)\n",
    "\n",
    "    # Plot learning graph through epochs, with accuracy and loss of train and validation set\n",
    "    _plot_history(model.history)\n",
    "\n",
    "    # Save final models\n",
    "    if not fn:\n",
    "        fn = get_timestamp()\n",
    "    \n",
    "    fp = f'models/cnn/{fn}.pth'\n",
    "    torch.save(model.module_.state_dict(), fp)\n",
    "    \n",
    "    print(f\"Best model saved to {fp}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Convolutional block for a layer of convolution followed by batch normalization, activation, and max pooling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs, outputs, activation=nn.GELU, kernel_size=3, \n",
    "                 padding='same', pool_kernel=3, pool_stride=2):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # Define convolutional layer\n",
    "        self.conv = nn.Conv1d(inputs, outputs, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "        # Batch normalization\n",
    "        self.bn = nn.BatchNorm1d(outputs)\n",
    "\n",
    "        # Activation function\n",
    "        self.activation = activation()\n",
    "\n",
    "        # Max pooling\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=pool_kernel, stride=pool_stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Forward through convolution, batch normalization, activation, and max pooling\n",
    "        x = self.activation(self.bn(self.conv(x)))\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Convolutional Neural Network model with multiple ConvBlocks followed by fully connected layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, neurons=128, activation=nn.GELU, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = ConvBlock(1, 32, activation=activation)\n",
    "        self.conv2 = ConvBlock(32, 64, activation=activation)\n",
    "        self.conv3 = ConvBlock(64, 128, activation=activation)\n",
    "        self.conv4 = ConvBlock(128, 256, activation=activation)\n",
    "        self.conv5 = ConvBlock(256, 512, activation=activation)\n",
    "\n",
    "        # Adaptive max pooling\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # Activation function\n",
    "        self.activation = activation()\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(512, neurons)\n",
    "\n",
    "        # Batch normalization\n",
    "        self.bn = nn.BatchNorm1d(neurons)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Output layer with 5 classes\n",
    "        self.fc2 = nn.Linear(neurons, 5)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Forward through convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        # Adaptive max pooling\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Flatten before passing to fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.activation(self.bn(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Residual block for a layer of convolution followed by activation and max pooling.\n",
    "        This structure is aimed to replicate the models done by M. Kachuee et al.\n",
    "        Further details will be described in glossary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1, padding='same', activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "        # Activation function\n",
    "        self.activation = activation()\n",
    "\n",
    "        # Max pooling layer\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=5, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Store the residual\n",
    "        residual = x\n",
    "\n",
    "        # Forward through the first convolutional layer and activation\n",
    "        out = self.conv1(x)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        # Forward through the second convolutional layer\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        # Add residual to output\n",
    "        out += residual\n",
    "\n",
    "        # Apply activation to output\n",
    "        out = self.activation(out)\n",
    "\n",
    "        # Apply max pooling\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ResCNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Residual Convolutional Neural Network model with multiple ResidualBlocks followed by fully connected layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the initial convolutional layer\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=5, stride=1)\n",
    "\n",
    "        # Define the sequence of residual blocks\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResidualBlock(32, 32, activation=activation),\n",
    "            ResidualBlock(32, 32, activation=activation),\n",
    "            ResidualBlock(32, 32, activation=activation),\n",
    "            ResidualBlock(32, 32, activation=activation),\n",
    "            ResidualBlock(32, 32, activation=activation)\n",
    "        )\n",
    "\n",
    "        # Flatten layer\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Activation function\n",
    "        self.activation = activation()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Forward through the initial convolutional layer\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # Forward through the sequence of residual blocks\n",
    "        x = self.res_blocks(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Softmax activation for multiclass classification\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-03-27 13:59:12 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def _svm_pipeline(balanced_sample=None, dimredc=None, \n",
    "                      n_components=None, n_folds=5, class_weight=None,\n",
    "                      decision_function_shape='ovr', model_fn=None,\n",
    "                      max_iter=-1):\n",
    "    \"\"\"\n",
    "        Encapsulated SVM pipeline to import data, preproces, \n",
    "        hyper paramater tuning, and training model\n",
    "                        \n",
    "        Params: \n",
    "        ------------------------------------------------------------------------------\n",
    "            balanced_sample - Number of records after balancing\n",
    "            dimredc - Feature reduction type (PCA, LDA)\n",
    "            n_components - Number of components will be retained after transformation\n",
    "            n_folds - Number of subsets that the dataset will be divided for cross-validation\n",
    "            class_weight - Whether to apply class weights or not\n",
    "            decision_function_shape - Shape of the decision function (ovo, ovr)\n",
    "            model_fn - Filename of final model\n",
    "            max_iter - Maximum number of iteration\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Import train data\n",
    "    X_train, y_train = _import_data('train')\n",
    "\n",
    "    # Balance data if specific\n",
    "    if balanced_sample:\n",
    "        X_train, y_train = _balancing(X_train, y_train, balanced_sample)\n",
    "    print(np.unique(y_train, return_counts=True))\n",
    "\n",
    "    # Feature reduction if specific\n",
    "    steps = []\n",
    "    if dimredc == 'pca':\n",
    "        steps.append(('pca', PCA(n_components=n_components)))\n",
    "    elif dimredc == 'lda':\n",
    "        steps.append(('lda', LinearDiscriminantAnalysis(n_components=n_components)))\n",
    "\n",
    "\n",
    "    # Initialize SVM model\n",
    "    steps.append(('svm', SVC(decision_function_shape=decision_function_shape, \n",
    "                             max_iter=max_iter, \n",
    "                             verbose=1,\n",
    "                             class_weight=class_weight)))\n",
    "    pipeline = Pipeline(steps)\n",
    "\n",
    "    # Perform stratified gridsearch cross validation\n",
    "    grid_search = GridSearchCV(pipeline, params, cv=StratifiedKFold(n_splits=n_folds), n_jobs=-1, scoring='f1_macro')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get best parameters\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "    # Define final model with best paramaters\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Save best model for further evaluation\n",
    "    if model_fn:\n",
    "        model_fp = f'models/svm/{model_fn}.pkl'\n",
    "    else:\n",
    "        model_fp = f'models/svm/{get_timestamp()}.pkl'\n",
    "\n",
    "    joblib.dump(best_model, model_fp)\n",
    "    print(f\"Model saved to {model_fp}\")\n",
    "\n",
    "    # Predict on train set, with classification report\n",
    "    y_pred = best_model.predict(X_train)\n",
    "    _get_report(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_svm(fn, subset='test', roc_curve=True):\n",
    "    \"\"\"\n",
    "        Evaluate SVM model\n",
    "                        \n",
    "        Params: \n",
    "        ------------------------------------------------------------------------------\n",
    "            fn - Filename of model to be evaluated\n",
    "            subset - Subset of source data to be evaluated (train, validation, test)\n",
    "            roc_curve - Whether to show ROC curve\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    if subset == 'test':\n",
    "        # Load data\n",
    "        X, y = _import_data('test')\n",
    "\n",
    "    else:\n",
    "        X, y = _import_data('train')\n",
    "\n",
    "    # Load best model\n",
    "    model = joblib.load(f\"models/svm/{fn}.pkl\")\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    # Evaluate, and predict\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    print(f\"Predicting time: {end-start}\")\n",
    "\n",
    "    # Get classification report\n",
    "    _get_report(y, y_pred)\n",
    "\n",
    "    # Generate confusion matric\n",
    "    _get_confusion_matrix(y, y_pred)\n",
    "\n",
    "    if roc_curve:\n",
    "\n",
    "        # Evaluate, and predict with probability\n",
    "        y_prob_test = model.predict_proba(X)\n",
    "\n",
    "        # Plot ROC curve\n",
    "        _roc_curve(y, y_prob_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
